# DocFace: Matching ID Document Photo to Selfies
## 背景
最近做人脸识别，现有的人脸数据中每个ID只有少数几张照片。每个ID可能包含一张证件照和一张或少数几张生活照。这样的数据使用classification based method不足以为每个ID提供足够的多样性。效果显然不会很好。于是就找到了这篇文章。人证核验。

每个ID人脸很少是一回事，证件照和生活照Domain不同又是一回事，显然这篇文章更关注后者。下图是证件照和生活照实例。

![图片](http://agroup-bos.cdn.bcebos.com/2fdf89f0af564b77663271b43fec819c7b65daa6)
## 核心
直接说重点，作者使用了两个同样结构的网络（不共享参数）分别提取证件照和生活照不同domain的特征，并且提出了*Max-margin Pariwise Score loss(MPS)*对特征施加约束。作者使用了公开数据集*MS-Celeb-1M*和*AM-softmax*训练的网络作为上述两个网络的初始化。*Transfer learning*，利用了大量数据下训练出来的底层特征。示意图：
![图片](http://agroup-bos.cdn.bcebos.com/66f2058aa8364ae743543e67d5d2258368d44451)
其中网络$G$用来提取证件照特征，$H$用来提取生活照特征。直接上损失函数：![图片](http://agroup-bos.cdn.bcebos.com/0a9383429669fe2d144ea4c325f434736f8e032f)
其中$$cos{\theta_{i,j}}= g^T_ih_j$$,$$g_i = \frac{G(x_{i1})}{||G(x_{i1}||^2_2}$$，$$h_i = \frac{H(x_{i2})}{||H(x_{i2}||^2_2}$$
$x_{i1},x_{i2}$分别表示第$i$对样本对证件照和生活照。这里强调以下，训练样本是同一个人的一对证件照和生活照组织的。分别解释上述损失函数的三项意义。如下

 1. 第一项，对所有**不匹配**的证件照和生活照pair,使用sibling网络分别提取特征计算他们之间的距离，选择距离最小（也就是余弦值最大）的进行优化。用人脸识别的话说就是‘最大化类间距离‘。
 2. 第二项就是对同一个的证件照和生活照进行约束，最小化其间距离。
 3. 第三项$m^/$ 类似于人脸识别损失函数里的margin，给予一个更大的惩罚。

最后的$+$号，$[x]_+ = max(0, x)$。即是说只惩罚使损失函数大于零的样本。有点semi-hard的意味。其实仔细想上面的损失函数，完全是和FaceNet一个套路，只不过本文更关注domain间的不同，所以使用了两个网络分别提取特征。

## 结论
![图片](http://agroup-bos.cdn.bcebos.com/0cd17d6513925d7350a5a9c253eb53c8e2b6836d)
最终结论如上图，其中*Base model*是用*AM-softmax*在*MS1m*上训练的网络，而DocFace是用私有训练集*finetune*而来。使用的测试集也是和作者私有训练集分布一致的数据。所以得出这个结果并不让我意外。这个对比也不是那么的公平。然而作者提出了一个全新的框架去利用了这样的人证数据，并且得到了一定的收益，还是有很大意义的。